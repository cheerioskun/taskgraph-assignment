# On Durability, Resilience and Resumability

## Durability
Durability refers to the ability of the system to preserve its state over time. The state in our system is:
- The structure of the task graph itself (nodes and edges)
- The state of each node (running, completed, failed)
- The outputs produced (or being produced) by each node at each invocation

Durability will be downstream from good data design. Storing the task graph itself is straightforward. What is tricky is the state of execution. A good approach is to take inspiration from the world of databases, and maintain a WAL (write-ahead log) that records events emitted at cutpoints between nodes.

Consider a set of events:
1. ExecutionStarted{executionid, workflowid, inputs}
2. NodeInvoked{nodeid, invocationid, inputs}
3. NodeCompleted{nodeid, invocationid, output}
4. NodeFailed{nodeid, invocationid, output, error}
5. ExecutionCompleted{executionid, workflowid, outputs}
6. ExecutionFailed{executionid, workflowid, outputs, error}

An ordered log of these events is sufficient to reconstruct the current frontier in the task graph, along with the outputs seeded into its execution. In a system where we have a slew of these workflows running over a fixed number of workers, we can further namespace these events by execution-id and workflow-id. Simply said, we can just put all these events into a DB, and have whatever scheduler distributes work also listen for ExecutionFailed events (along with NodeInvoked events), to invoke the start node for the workflow again.

The workers simply get assigned a node to execute, they pull the actual information about the node from some common place, then work on it and emit events as they go. The workers themselves can be stateless, and the system can scale horizontally. This works fine in face of failures within the workers, but there is a missing piece in case of failures of the workers themselves. If a worker fails mid-execution, we need to ensure that NodeFailed events are emitted. This can be done by having a heartbeat mechanism, where the worker periodically emits a heartbeat event. If the scheduler does not see a heartbeat for a given node within a timeout period, it can assume that the worker has failed and emit a NodeFailed event for that node.

7. NodeHeartbeat{nodeid, invocationid, timestamp}

Given this framework, the durability of the system is ensured by the durability of the event log. Making the even log durable is mostly a matter of good engineering and using the right tools.

## Resilience
Our current system should get us quite far. It is resilient to both worker failures and node failures. We do still need to have a mechanism to not retry failed nodes into eternity. This can be done by having a retry count associated with each node or even a human in the loop mechanism.

## Resumability
To talk about resumability, we need to define a resolution. At the resolution of a node, we can resume the execution of a workflow from the last failed node. This is because we have the entire state of the workflow in the event log. We can simply replay the events from the log to reconstruct the state of the workflow up to the point of failure, and then continue executing from there.
At a finer resolution within the node, what the platform can do is provide a mechanism for the node to checkpoint its state. This is non trivial. One method is to allow checkpointing and also provide the checkpoint data as part of the node invocation. The node then needs to have the logic to properly use this checkpoint data to resume its execution. This is a more complex mechanism and requires more work from the node implementer, but it can be very useful for long running nodes.

Here it is pragmatic to use Temporal as is, since they provide a lot of what we want out of the box. We just need to massage our structures to work with their SDK. At the least, we could/should explore the specifics of how temporal allows for resumability within a node/activity, or does it also only allow resuming at the activity boundary.
